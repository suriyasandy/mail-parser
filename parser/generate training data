import pandas as pd
import spacy
from spacy.tokens import DocBin

# Fields to extract as NER entities
ENTITY_FIELDS = {
    "trade_id": "TRADE_ID",
    "pnl_amount": "PNL",
    "commission_amount": "COMMISSION",
    "deviation_percent": "DEVIATION",
    "alert_description": "ALERT_DESC",
    "legal_entity": "LEGAL_ENTITY",
    "trade_date": "TRADE_DATE",
    "settlement_date": "SETTLEMENT_DATE",
    "notional_range": "NOTIONAL",
    "base_threshold_percent": "BASE_THRESHOLD",
    "orig_threshold_percent": "ORIG_THRESHOLD"
}

def generate_training_data(df: pd.DataFrame):
    training_data = []

    for _, row in df.iterrows():
        try:
            sentence = (
                f"Trade {row['trade_id']} was executed for legal entity {row['legal_entity']} "
                f"on {row['trade_date']} and settled on {row['settlement_date']}. "
                f"The PnL was {row['pnl_amount']} with a commission of {row['commission_amount']}. "
                f"Deviation percent was {row['deviation_percent']}. "
                f"Alert description: {row['alert_description']}. "
                f"Notional range was {row['notional_range']} with base threshold {row['base_threshold_percent']} "
                f"and original threshold {row['orig_threshold_percent']}."
            )

            entities = []
            for col, label in ENTITY_FIELDS.items():
                val = str(row[col])
                start = sentence.find(val)
                if start != -1:
                    end = start + len(val)
                    entities.append((start, end, label))

            training_data.append((sentence, {"entities": entities}))

        except Exception as e:
            print(f"Skipping row due to error: {e}")
    
    return training_data

import spacy
from spacy.tokens import DocBin

def save_as_spacy(training_data, output_path, lang="en"):
    """
    Converts the training data into spaCy binary format (.spacy), avoiding overlapping entities.

    Args:
        training_data: List of (text, {"entities": [(start, end, label), ...]})
        output_path: Output path for the .spacy file
        lang: Language code (default: "en")
    """
    nlp = spacy.blank(lang)
    doc_bin = DocBin()

    for text, annot in training_data:
        doc = nlp.make_doc(text)
        ents = []
        seen_tokens = set()

        for start, end, label in annot["entities"]:
            span = doc.char_span(start, end, label=label)
            if span is not None:
                span_token_indices = set(range(span.start, span.end))
                if not seen_tokens.intersection(span_token_indices):
                    ents.append(span)
                    seen_tokens.update(span_token_indices)

        doc.ents = ents
        doc_bin.add(doc)

    doc_bin.to_disk(output_path)
    print(f"âœ… Saved training data to: {output_path}")
