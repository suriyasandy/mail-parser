import streamlit as st
import pandas as pd

from extraction.msg_parser_util import parse_msg_file_categorized
from extraction.cleaner import clean_text
from extraction.attribute_extractor import extract_attributes_from_text, detect_language
from extraction.table_extractor import extract_tables_from_html, extract_plaintext_tables
from extraction.ocr_extractor import extract_text_from_images
from extraction.pdf_extractor import extract_text_from_pdf
from extraction.attachment_extractor import extract_from_csv, extract_from_xlsx
from utils.save_feedback import save_feedback
from ner.retrain import train_model

from utils.keyword_extractor import extract_keywords
from nlp.sentence_splitter import split_into_sentences

st.set_page_config(page_title="Trade Entity Extraction", layout="wide")

# --- Sidebar: Role Selection ---
st.sidebar.title("User Role")
role = st.sidebar.selectbox("Select role", ["Analyst", "Supervisor", "Admin"])
st.sidebar.write(f"Role: {role}")

st.title("Email-based Trade Entity Extraction System")

# --- Inputs ---
uploaded = st.file_uploader("Upload .msg file", type=["msg"])
demo_text = st.text_area("‚Äîor paste email text‚Äî", height=150)

def summarize_by_keywords(text: str, top_n_sentences: int = 3) -> list[str]:
    # 1) Extract global keywords
    kws = extract_keywords(text, top_n=10)
    # 2) Split into sentences
    sents = split_into_sentences(text)
    # 3) Score sentences by keyword hits
    scored = [(sent, sum(1 for kw in kws if kw.lower() in sent.lower())) for sent in sents]
    # 4) Pick top N
    scored.sort(key=lambda x: x[1], reverse=True)
    top_set = {s for s,_ in scored[:top_n_sentences]}
    # 5) Return in original order
    return [s for s in sents if s in top_set]

if uploaded or demo_text.strip():
    # 1) Parse & categorize
    if uploaded:
        parsed = parse_msg_file_categorized(uploaded)
        text_body        = parsed["text_body"]
        html_tables      = parsed["html_tables"]
        plaintext_tables = parsed["plaintext_tables"]
        ocr_results      = parsed["ocr_results"]
        attachments      = parsed.get("attachments", [])
    else:
        text_body        = demo_text
        html_tables      = []
        plaintext_tables = []
        ocr_results      = []
        attachments      = []

    # 2) Clean text body
    cleaned = clean_text(text_body)
    st.subheader("Cleaned Email Text")
    st.code(cleaned)

    # 3) Thread summary (keyword-driven)
    st.subheader("üìÑ Thread Summary")
    summary_sents = summarize_by_keywords(cleaned, top_n_sentences=3)
    if summary_sents:
        for s in summary_sents:
            st.write("‚Ä¢ " + s)
    else:
        st.write("‚ÄîNo summary available‚Äî")

    # 4) Global keywords (RAKE)
    global_kws = extract_keywords(cleaned, top_n=10)
    st.subheader("üîë Predicted Keywords")
    st.write(global_kws)

    # Helper to flag important tables
    def is_important(df: pd.DataFrame) -> bool:
        text = " ".join(df.columns.astype(str).tolist() +
                        df.astype(str).values.flatten().tolist()).lower()
        tbl_kws = set(extract_keywords(text, top_n=5))
        return bool(tbl_kws.intersection(map(str.lower, global_kws)))

    # 5) Important HTML tables
    imp_html = [df for df in html_tables if is_important(df)]
    st.subheader("üîç Important HTML Tables")
    if imp_html:
        for i, df in enumerate(imp_html, 1):
            st.write(f"Table {i}")
            st.dataframe(df)
    else:
        st.info("No important HTML tables detected.")

    # 6) Important plaintext tables
    imp_pt = [df for df in plaintext_tables if is_important(df)]
    st.subheader("üîç Important Plaintext Tables")
    if imp_pt:
        for i, df in enumerate(imp_pt, 1):
            st.write(f"Table {i}")
            st.dataframe(df)
    else:
        st.info("No important plain-text tables detected.")

    # 7) Unified extraction & validation
    all_entities = []

    # inline attributes
    for a in extract_attributes_from_text(cleaned):
        all_entities.append({
            "attribute": a["attribute"],
            "value":     a["value"],
            "source":    "inline_text"
        })

    # tables (all HTML + plaintext)
    for i, df in enumerate(html_tables, 1):
        for _, row in df.iterrows():
            for col, val in row.items():
                all_entities.append({
                    "attribute": col, "value": val,
                    "source":    f"html_tbl_{i}"
                })
    for i, df in enumerate(plaintext_tables, 1):
        for _, row in df.iterrows():
            for col, val in row.items():
                all_entities.append({
                    "attribute": col, "value": val,
                    "source":    f"plain_tbl_{i}"
                })

    # OCR on images
    for res in ocr_results:
        fname = res["filename"]
        if "error" in res:
            continue
        ocr_txt = clean_text(res["text"])
        for a in extract_attributes_from_text(ocr_txt):
            all_entities.append({
                "attribute": a["attribute"],
                "value":     a["value"],
                "source":    f"ocr_{fname}"
            })

    # attachments (PDF/CSV/Excel)
    for att in attachments:
        name = att.get("filename","")
        data = att.get("data")
        if name.lower().endswith(".pdf"):
            for pg in extract_text_from_pdf(data):
                txt = clean_text(pg.get("text","") or "")
                for a in extract_attributes_from_text(txt):
                    all_entities.append({
                        "attribute": a["attribute"],
                        "value":     a["value"],
                        "source":    f"pdf_{name}_p{pg['page']}"
                    })
        elif name.lower().endswith(".csv"):
            df = extract_from_csv(data)
            for _, row in df.iterrows():
                for col, val in row.items():
                    all_entities.append({
                        "attribute": col,
                        "value":     val,
                        "source":    f"csv_{name}"
                    })
        elif name.lower().endswith((".xls", ".xlsx")):
            df = extract_from_xlsx(data)
            for _, row in df.iterrows():
                for col, val in row.items():
                    all_entities.append({
                        "attribute": col,
                        "value":     val,
                        "source":    f"xlsx_{name}"
                    })

    # 8) Display & review
    if all_entities:
        st.header("üîÄ Merged Extraction Results")
        df_ent = pd.DataFrame(all_entities)
        df_ent["valid"]           = False
        df_ent["edited_attribute"]= df_ent["attribute"]
        df_ent["edited_value"]    = df_ent["value"]

        for i, row in df_ent.iterrows():
            c1, c2, c3, c4 = st.columns([2,2,1,3])
            df_ent.at[i,"edited_attribute"] = c1.text_input(
                f"Attr {i}", row["attribute"], key=f"a_{i}"
            )
            df_ent.at[i,"edited_value"]    = c2.text_input(
                f"Val {i}",   row["value"],     key=f"v_{i}"
            )
            df_ent.at[i,"valid"]           = c3.checkbox(
                "Valid", key=f"c_{i}"
            )
            c4.write(row["source"])

        if role in ["Analyst","Supervisor","Admin"]:
            valid = df_ent[df_ent["valid"]]
            if not valid.empty:
                csv_bytes = valid[["edited_attribute","edited_value"]].to_csv(index=False).encode()
                st.download_button("Download CSV", data=csv_bytes, file_name="entities.csv")
            if role in ["Supervisor","Admin"] and st.button("Submit Feedback"):
                feedback = [
                    {"label":r["edited_attribute"],"text":r["edited_value"]}
                    for _,r in valid.iterrows()
                ]
                save_feedback(cleaned, feedback)
                st.success("Feedback saved.")
        if role=="Admin" and st.button("Retrain Model"):
            train_model()
            st.success("Model retrained.")
    else:
        st.info("No entities extracted.")
